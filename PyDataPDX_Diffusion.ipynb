{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "143008de-3fbf-4c63-9982-8a5bc8f62f4a",
      "metadata": {
        "id": "143008de-3fbf-4c63-9982-8a5bc8f62f4a"
      },
      "source": [
        "# Diffusion Models\n",
        "\n",
        "\"Abraham Lincoln underwater playing the accordion, impressionist painting in the style of Monet\" rendered by Stable Diffusion\n",
        "\n",
        "<img src=\"https://i.imgur.com/2DattEn.png\" width=400><br/>\n",
        "\n",
        "*PyData PDX - October 12, 2022*\n",
        "\n",
        "* Background\n",
        "  * Sampling problem\n",
        "  * Manifold hypothesis\n",
        "  * Learned embeddings; latent spaces\n",
        "* From GANs to recent diffusers\n",
        "  * The foundations of modern text-to-image: VQGAN, CLIP\n",
        "  * From CLIP+VQGAN to Stable Diffusion\n",
        "  * Energy-based models, score-based models\n",
        "  * General structure of recent diffusion models\n",
        "* Building and sampling from a toy diffuser model\n",
        "* Interpreting the Stable Diffusion paper diagram\n",
        "* What we don’t understand: recent surprises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c045f48b-580b-4795-b9c8-66d51226a61a",
      "metadata": {
        "editable": true,
        "id": "c045f48b-580b-4795-b9c8-66d51226a61a"
      },
      "source": [
        "## Background\n",
        "\n",
        "Today, we're not surprised or unreasonably impressed with synthetic images like these:\n",
        "\n",
        "<img src=\"https://materials.s3.amazonaws.com/i/gan.jpg\"><br/>\n",
        "\n",
        "And we have access to even larger, better quality images via sites like\n",
        "* https://thispersondoesnotexist.com/\n",
        "* https://thiscatdoesnotexist.com/\n",
        "\n",
        "These creations are the result of the GAN deep-learning pattern for image synthesis.\n",
        "\n",
        "## Image generation as sampling\n",
        "\n",
        "Image generation can be seen as a form of sampling from the distribution of \"legitimate\" images or conditional distribution of legitimate images corresponding to a prompt, class, etc.\n",
        "\n",
        "### Key challenges and concepts\n",
        "\n",
        "* Arbitrary and high-dimensional nature of pixel space\n",
        "* Manifold hypothesis\n",
        "* Embeddings\n",
        "  * What are they?\n",
        "  * Where do they get semantics from?\n",
        "  * How/why use them?\n",
        "\n",
        "### A few sampling mechanisms\n",
        "\n",
        "* Rejection sampling\n",
        "* Closed-form or simple approximation to transform samples from an easy distribution to a less-easy one\n",
        "    * e.g., uniform to Gaussian \n",
        "    * via approximate inverse-CDF\n",
        "* Variational methods (approximate the true distribution with one that's easier to manipulate and sample from)\n",
        "* Markov chain Monte Carlo (MCMC) methods like Gibbs, Metropolis-Hastings, Hamiltonian, etc.\n",
        "\n",
        "### Where do GANs fit in?\n",
        "\n",
        "GANs\n",
        "* are related to the second option above ... but on \"ultra-hard mode\" (neither a meaningful closed form nor simple approximation)\n",
        "* use neural networks and a particular training algorithm to fit the transformation function\n",
        "* yield interesting and useful compressed representations in embedding or latent space\n",
        "* once trained, require just a single forward pass to move from a simple distribution (typically a high-dimensional Gaussian) to an output (usually an image) -- no MCMC\n",
        "* Characteristic which defines GANs:\n",
        "  * The \"A\" in GAN -- i.e., the adversarial training algorithm\n",
        "  * Adversarial algorithm is also a vulnerability which can make training difficult"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac3a1eb-5b73-4b36-9a35-d8d8b187e8fc",
      "metadata": {
        "id": "bac3a1eb-5b73-4b36-9a35-d8d8b187e8fc"
      },
      "source": [
        "## From GANs to modern diffusers\n",
        "\n",
        "<img src='https://materials.s3.amazonaws.com/i/astro.png' width=500 />\n",
        "\n",
        "Perhaps the MNIST of 2022, the astronaut riding a horse has become emblematic of modern text-prompted diffusion models. We won't build a model of this quality here (for obvious reasons) but if you want to make your own equestrian spacemen or see a few pre-made specimens, it doesn't get easier than this notebook using Huggingface wrappers for accessing Stable Diffusion: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057a743a-3683-45d4-b086-b8961ad7cfda",
      "metadata": {
        "id": "057a743a-3683-45d4-b086-b8961ad7cfda"
      },
      "source": [
        "__Catching up__\n",
        "\n",
        "Compressing an amazing amount of brilliant work into a tl;dr, the last couple of years look like this...\n",
        "\n",
        "* VAE (prior work)\n",
        "* VQVAE: VAE + quantized vectors (categorical/discrete)  \n",
        "  * https://arxiv.org/pdf/1711.00937.pdf\n",
        "  * Extensive explanation at https://ml.berkeley.edu/blog/posts/vq-vae/\n",
        "  * DALL-E ~ Transformer (autoregressive generation) + VQVAE\n",
        "* VQGAN ~ VQVAE discrete codebook idea + CNN + Transformers (on the image side!) + patch sequences + dominant perceptual loss\n",
        "  * https://arxiv.org/pdf/2012.09841.pdf\n",
        "* CLIP ~ image captioning by jointly training a text encoder and image encoder (https://arxiv.org/pdf/2103.00020.pdf)\n",
        "\n",
        "__CLIP + VQGAN__\n",
        "\n",
        "Pre-trained CLIP and pre-trained VQGAN\n",
        "\n",
        "> We start with a text prompt and use a GAN to iteratively generate candidate images, at each step using CLIP to improve the image. We optimize the image by treating the squared spherical distance between the embedding of the candidate and the embedding of the text prompt as a loss function, and differentiating through CLIP with respect to the GAN’s latent vector representation\n",
        "of the image (https://arxiv.org/abs/2204.08583)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd8870de-ef3f-4c3a-bc6b-af3eb5075254",
      "metadata": {
        "id": "cd8870de-ef3f-4c3a-bc6b-af3eb5075254"
      },
      "source": [
        "__... getting to Stable Diffusion__\n",
        "\n",
        "* Dall-e-2\n",
        "  * CLIP-based with either (autoregressive or a flavor of __latent diffusion__) + (diffusion upsample) to generate encoded image vector (which is then expanded)\n",
        "  * https://cdn.openai.com/papers/dall-e-2.pdf\n",
        "  * You may have played with Dall-e-mini [a.k.a Craiyon - https://www.craiyon.com/] or at least seen it on social media (https://github.com/borisdayma/dalle-mini)\n",
        "    * Here is a small, more accessible port: https://github.com/kuprel/min-dalle\n",
        "* Parti\n",
        "  * Based on improved VQGAN\n",
        "  * https://parti.research.google/\n",
        "  * https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html\n",
        "* Imagen\n",
        "  * Diffusion model with pre-trained LLM input\n",
        "  * https://arxiv.org/abs/2205.11487\n",
        "  * https://www.assemblyai.com/blog/how-imagen-actually-works/\n",
        "\n",
        "Nice summary: https://twitter.com/savvyrl/status/1540555792331378688\n",
        "\n",
        "We'll drill down on __diffusion models__ in a moment.\n",
        "\n",
        "For context/situational awareness: once we have diffusion models, two more bits get us to Stable Diffusion, an open model based on Dall-e-2 concepts...\n",
        "\n",
        "* Guided diffusion\n",
        "  * More detail on class conditioning vs. classifier-free guidance for diffusion models (https://arxiv.org/abs/2207.12598)\n",
        "* Latent guided diffusion (https://ommer-lab.com/research/latent-diffusion-models/)\n",
        "  * Diffusion in the latent space instead of in the pixel space\n",
        "  \n",
        "We'll come back to closer look at Stable Diffusion but first..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79de5431-39a6-4af0-8aa2-ef7e7ac97e5b",
      "metadata": {
        "id": "79de5431-39a6-4af0-8aa2-ef7e7ac97e5b"
      },
      "source": [
        "### Code detail: diffusion model\n",
        "\n",
        "If you've looked at any discussion of diffusion models, you've probably seen an image like this...\n",
        "\n",
        "<img src='https://cdn-images-1.medium.com/max/820/1*RDPhd2dvmHE4UrAP-QHb9w.png' />\n",
        "\n",
        "It's a cool idea ... but suggests a lot of questions:\n",
        "* Why would we ever expect this sort of thing to work?\n",
        "  * If you've seen denoising networks, you might instead say, \"I buy the part on the left ... but how does the 'reverse' process start?\"\n",
        "* If it appears to work for some samples ... can we rely on it to cover the target distribution well?\n",
        "* Is it efficient (computationally)?\n",
        "\n",
        "Diffusion models are a form of energy-based model -- models inspired by physical knowledge (typically but not always from statistical mechanics) of how certain processes involving adjusting energy and entropy levels systematically can produce a known set of world states. See https://towardsdatascience.com/the-physics-of-energy-based-models-1121122d0d9\n",
        "\n",
        "That's a mouthful... so let's try to explain a bit more.\n",
        "\n",
        "One very classic example you may have seen or implemented is simulated annealing (https://en.wikipedia.org/wiki/Simulated_annealing)\n",
        "\n",
        "Another case, a bit closer to deep learning, is the Hopfield network. The late David MacKay's coverage (from the epic *Information Theory, Inference, and Learning Algorithms*): http://www.inference.org.uk/mackay/itprnn/ps/504.520.pdf\n",
        "\n",
        "In our case, where we are looking not for a single optimum, but a complex distribution, what we want is a\n",
        "* training mechanism that places the target (training) distribution at low-energy places in the parameter-energy landscape\n",
        "* sampling mechanism that can provably (or at least practically) yield low-energy configurations\n",
        "* justification for believing the output configurations are distributed properly\n",
        "  * in the simple case, this is avoiding mode collapse\n",
        "  * in the broader case, this is accessing all parts of the training distribution with the relevant likelihoods\n",
        "  \n",
        "Key papers include\n",
        "* Ho et al. -- https://arxiv.org/abs/2006.11239 -- from which we'll get the algorithm used below\n",
        "* and Song et al. -- https://arxiv.org/abs/2011.13456 \n",
        "  * which is recapped in the excellent blog \"Generative Modeling by Estimating Gradients of the Data Distribution\" (by Yang Song) at https://yang-song.net/blog/2021/score/\n",
        "  \n",
        "This blog post -- https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ -- which provides more detail (and math) than we have time for today, is not just a great explainer on diffusion models -- its list of citations is a \"greatest hits\" of recent work linking diffusion, score-based models, Langevin dynamics, and efficient approximations of target processes. \n",
        "\n",
        "Essentially, we want not just a method, but also its justification, scope, and limitations.\n",
        "\n",
        "__Context__\n",
        "\n",
        "To set the scene, we want\n",
        "* a training loop that learns a denoising-step function for our specific training set\n",
        "* a sampling loop that uses the trained function together with noise injection to move toward the target distribution\n",
        "\n",
        "Let's walk through a minimal implementation by Francois Fleuret. The full code is at https://fleuret.org/git-extract/pytorch/minidiffusion.py but here we've trimmed it down so we can highlight the critical bits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66030dc5-223e-485a-8014-aa5427bfdb79",
      "metadata": {
        "id": "66030dc5-223e-485a-8014-aa5427bfdb79"
      },
      "outputs": [],
      "source": [
        "# Any copyright is dedicated to the Public Domain.\n",
        "# https://creativecommons.org/publicdomain/zero/1.0/\n",
        "\n",
        "# Written by Francois Fleuret <francois@fleuret.org>\n",
        "\n",
        "import torch\n",
        "import math, argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d481aa2-d876-47f3-8195-a94dce9069df",
      "metadata": {
        "id": "1d481aa2-d876-47f3-8195-a94dce9069df"
      },
      "source": [
        "For training purposes, we need data samples. Here we'll generate them ourselves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b550135a-e180-43f4-b675-7c9f897f3bca",
      "metadata": {
        "id": "b550135a-e180-43f4-b675-7c9f897f3bca"
      },
      "outputs": [],
      "source": [
        "def sample_disc_grid(nb):\n",
        "    a = torch.rand(nb) * math.pi * 2\n",
        "    b = torch.rand(nb).sqrt()\n",
        "    N = 4\n",
        "    q = (torch.randint(N, (nb,)) - (N - 1) / 2) / ((N - 1) / 2)\n",
        "    r = (torch.randint(N, (nb,)) - (N - 1) / 2) / ((N - 1) / 2)\n",
        "    b = b * 0.1\n",
        "    result = torch.empty(nb, 2)\n",
        "    result[:, 0] = a.cos() * b + q\n",
        "    result[:, 1] = a.sin() * b + r\n",
        "    return result\n",
        "\n",
        "def sample_spiral(nb):\n",
        "    u = torch.rand(nb)\n",
        "    rho = u * 0.65 + 0.25 + torch.rand(nb) * 0.15\n",
        "    theta = u * math.pi * 3\n",
        "    result = torch.empty(nb, 2)\n",
        "    result[:, 0] = theta.cos() * rho\n",
        "    result[:, 1] = theta.sin() * rho\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ab3992-79cc-4de2-b6d0-ad10565b1629",
      "metadata": {
        "id": "42ab3992-79cc-4de2-b6d0-ad10565b1629"
      },
      "source": [
        "Gather a minimum set of configs/hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff2d1f0-5e92-4f34-8c63-5c5a270ce605",
      "metadata": {
        "id": "cff2d1f0-5e92-4f34-8c63-5c5a270ce605"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.argv = ['']\n",
        "\n",
        "parser = argparse.ArgumentParser(\n",
        "    description = '''A minimal implementation of Jonathan Ho, Ajay Jain, Pieter Abbeel\n",
        "      \"Denoising Diffusion Probabilistic Models\" (2020) https://arxiv.org/abs/2006.11239''',\n",
        "    formatter_class = argparse.ArgumentDefaultsHelpFormatter\n",
        ")\n",
        "\n",
        "parser.add_argument('--seed', type = int, default = 0, help = 'Random seed, < 0 is no seeding')\n",
        "parser.add_argument('--nb_epochs', type = int, default = 40, help = 'How many epochs')\n",
        "parser.add_argument('--batch_size', type = int, default = 25, help = 'Batch size')\n",
        "parser.add_argument('--nb_samples', type = int, default = 25000, help = 'Number of training examples')\n",
        "parser.add_argument('--learning_rate', type = float, default = 1e-3, help = 'Learning rate')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "if args.seed >= 0:\n",
        "    torch.manual_seed(args.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed3ff34-cdec-40cc-916a-04eba3ebc58f",
      "metadata": {
        "id": "fed3ff34-cdec-40cc-916a-04eba3ebc58f"
      },
      "source": [
        "Now we'll get a set of training samples.\n",
        "\n",
        "We'll also capture the means and stds of the inputs, which will be critical later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5eaa54b-b627-42f3-b22d-4bf8f9a8de1d",
      "metadata": {
        "id": "e5eaa54b-b627-42f3-b22d-4bf8f9a8de1d"
      },
      "outputs": [],
      "source": [
        "# train_input = sample_disc_grid(args.nb_samples).to(device)\n",
        "train_input = sample_spiral(args.nb_samples).to(device)\n",
        "\n",
        "train_mean, train_std = train_input.mean(), train_input.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b195d36a-92a7-4a4c-80ba-eceffe55594a",
      "metadata": {
        "id": "b195d36a-92a7-4a4c-80ba-eceffe55594a"
      },
      "source": [
        "The neural net we'll use here is intentionally simple. The only unusual piece is the `TimeAppender` which adds a new dimension to capture the time step at which we're operating ... the time step itself is an important parameter. I.e., the network is allowed to know where it is in the diffusion process, a key bit of \"special sauce\" for making it work.\n",
        "\n",
        "We'll define the `TimeAppender` then the network which uses it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be896e8-4248-4c01-9f2e-36515f2b2f7d",
      "metadata": {
        "id": "2be896e8-4248-4c01-9f2e-36515f2b2f7d"
      },
      "outputs": [],
      "source": [
        "# Gets a pair (x, t) and appends t (scalar or 1d tensor) to x as an additional dimension / channel\n",
        "\n",
        "class TimeAppender(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, u):\n",
        "        x, t = u\n",
        "        if not torch.is_tensor(t):\n",
        "            t = x.new_full((x.size(0),), t)\n",
        "        t = t.view((-1,) + (1,) * (x.dim() - 1)).expand_as(x[:,:1])\n",
        "        return torch.cat((x, t), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0399924-758d-4071-971e-25ce14d7de85",
      "metadata": {
        "id": "e0399924-758d-4071-971e-25ce14d7de85"
      },
      "source": [
        "What would it look like to pass some samples and a time step through this module?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2720319-8030-413a-acab-c46eb9ae1927",
      "metadata": {
        "id": "b2720319-8030-413a-acab-c46eb9ae1927"
      },
      "outputs": [],
      "source": [
        "demo = sample_spiral(5)\n",
        "demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd56d4e-906a-4219-8b53-9b66c6e01963",
      "metadata": {
        "id": "bcd56d4e-906a-4219-8b53-9b66c6e01963"
      },
      "outputs": [],
      "source": [
        "demo_ta = TimeAppender()\n",
        "t_0 = 17\n",
        "\n",
        "demo_ta((demo, t_0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7087fe-e821-4aee-86a6-0770a365c0f8",
      "metadata": {
        "id": "6b7087fe-e821-4aee-86a6-0770a365c0f8"
      },
      "outputs": [],
      "source": [
        "nh = 256\n",
        "\n",
        "model = nn.Sequential(\n",
        "    TimeAppender(),\n",
        "    nn.Linear(train_input.size(1) + 1, nh),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(nh, nh),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(nh, nh),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(nh, train_input.size(1)),\n",
        ")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b811622d-896a-4358-a790-3212b8da32ff",
      "metadata": {
        "id": "b811622d-896a-4358-a790-3212b8da32ff"
      },
      "source": [
        "The training loop will use a set of values for adjusting energy levels (noise mixing) based on a schedule\n",
        "* with 1000 total steps (\"T\")\n",
        "* linearly spaced from 1e-4 to 0.02\n",
        "* a handful of derived quantities\n",
        "  * including a set of precomputed cumulative products (log-sum-exp) that allow direct access to arbitrary steps in the noising process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a187645a-2582-49d2-94af-122784708a56",
      "metadata": {
        "id": "a187645a-2582-49d2-94af-122784708a56"
      },
      "outputs": [],
      "source": [
        "T = 1000\n",
        "beta = torch.linspace(1e-4, 0.02, T)\n",
        "alpha = 1 - beta\n",
        "alpha_bar = alpha.log().cumsum(0).exp()\n",
        "sigma = beta.sqrt()\n",
        "\n",
        "plt.plot(beta, label='beta')\n",
        "plt.plot(alpha, label='alpha')\n",
        "plt.plot(alpha_bar, label='alpha_bar')\n",
        "plt.plot(sigma, label='sigma')\n",
        "plt.yscale('log')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "400faabe-0122-43e2-bf29-a9ba613fb762",
      "metadata": {
        "id": "400faabe-0122-43e2-bf29-a9ba613fb762"
      },
      "outputs": [],
      "source": [
        "demo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7517da-abb1-43bb-8fef-01a5b3b2b597",
      "metadata": {
        "id": "bd7517da-abb1-43bb-8fef-01a5b3b2b597"
      },
      "source": [
        "We're going to sample time steps and make a tensor with additional axes so that scalars from the energy schedule get broadcast across a tensor that matches the data dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11cfc181-efef-42d0-9e17-0eace8b17653",
      "metadata": {
        "id": "11cfc181-efef-42d0-9e17-0eace8b17653"
      },
      "outputs": [],
      "source": [
        "t_demo = torch.randint(T, (demo.size(0),) + (1,) * (demo.dim()-1)) #1 below\n",
        "\n",
        "t_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1360a2c-dff0-4f47-9063-168e3882ec0d",
      "metadata": {
        "id": "f1360a2c-dff0-4f47-9063-168e3882ec0d"
      },
      "outputs": [],
      "source": [
        "torch.randint(T, (demo.size(0),) + (1,) * (demo.dim()-1)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6753b683-8939-4065-a202-bb4a21836bb8",
      "metadata": {
        "id": "6753b683-8939-4065-a202-bb4a21836bb8"
      },
      "outputs": [],
      "source": [
        "demo_2 = torch.ones(5,2)\n",
        "\n",
        "demo_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07cf90be-c708-485c-9832-ce6e2c3c3459",
      "metadata": {
        "id": "07cf90be-c708-485c-9832-ce6e2c3c3459"
      },
      "outputs": [],
      "source": [
        "alpha_bar[17] * demo_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99b6e2a-e512-42cf-ae6d-2956edf7730c",
      "metadata": {
        "id": "f99b6e2a-e512-42cf-ae6d-2956edf7730c"
      },
      "outputs": [],
      "source": [
        "alpha_bar[t_demo] * demo_2 #2 below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "325b1bd7-fea9-41e2-ab16-ccbc21ddd605",
      "metadata": {
        "id": "325b1bd7-fea9-41e2-ab16-ccbc21ddd605"
      },
      "source": [
        "Those tensors will be used for mixing -- part training sample and part Gaussian noise.\n",
        "\n",
        "We'll then pass that mixture and an explicit scaled version of the timestep through the model ... and minimize the MSE of a noise prediction.\n",
        "\n",
        "We're going to implement the training algorithm (and later the sampling algorithm) per Ho et al.:\n",
        "\n",
        "<img src='https://materials.s3.amazonaws.com/i/ho-alg.png' width=700 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3225651-627b-4b4c-8718-0e137195405b",
      "metadata": {
        "id": "e3225651-627b-4b4c-8718-0e137195405b"
      },
      "outputs": [],
      "source": [
        "alpha_bar = alpha_bar.to(device)\n",
        "\n",
        "for k in range(args.nb_epochs):\n",
        "\n",
        "    acc_loss = 0\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate)\n",
        "\n",
        "    for x0 in train_input.split(args.batch_size):\n",
        "        x0 = (x0 - train_mean) / train_std # standardize\n",
        "        t = torch.randint(T, (x0.size(0),) + (1,) * (x0.dim() - 1), device = x0.device) #1 sample timesteps\n",
        "        eps = torch.randn_like(x0).to(device) # sample Gaussian\n",
        "        xt = torch.sqrt(alpha_bar[t]) * x0 + torch.sqrt(1 - alpha_bar[t]) * eps #2 mix\n",
        "        output = model((xt, t / (T - 1) - 0.5))\n",
        "        loss = (eps - output).pow(2).mean() # MSE to learn eps (noise)\n",
        "        acc_loss += loss.item() * x0.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'{k} {acc_loss / train_input.size(0)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73804f8b-a47c-4e46-a955-b964d7def12c",
      "metadata": {
        "id": "73804f8b-a47c-4e46-a955-b964d7def12c"
      },
      "source": [
        "At this point we have a denoising model that should capture the training distribution.\n",
        "\n",
        "If we were using a \"real\" model ... say, Stable Diffusion, this is the point we'd get to by downloading the pre-trained weight checkpoint. The distribution is \"in\" the model ... but we still need a pseudo-MCMC process to find samples.\n",
        "\n",
        "We'll start with a Gaussian vector and repeatedly run it through our noise predictor,\n",
        "* each time removing a scaled amount of the predicted noise\n",
        "* adding new noise (per the sigma schedule)\n",
        "\n",
        "Reminder: here, the time step (\"T\") counts down -- that's what makes this process go \"backwards\" even though the model always goes in one direction, from (image, timestep) -> noise estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f3d431-0447-44fe-8e23-2693a0e887aa",
      "metadata": {
        "id": "61f3d431-0447-44fe-8e23-2693a0e887aa"
      },
      "outputs": [],
      "source": [
        "def generate(size, T, alpha, alpha_bar, sigma, model, train_mean, train_std):\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        x = torch.randn(size, device = device)\n",
        "\n",
        "        for t in range(T-1, -1, -1):\n",
        "            output = model((x, t / (T - 1) - 0.5))\n",
        "            z = torch.zeros_like(x) if t == 0 else torch.randn_like(x)\n",
        "            x = 1/torch.sqrt(alpha[t]) \\\n",
        "                * (x - (1-alpha[t]) / torch.sqrt(1-alpha_bar[t]) * output) \\\n",
        "                + sigma[t] * z\n",
        "\n",
        "        x = x * train_std + train_mean\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "x = generate((1200, 2), T, alpha, alpha_bar, sigma, model, train_mean, train_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d96092-26f0-4f3c-ae23-dfd1f2e4f5ff",
      "metadata": {
        "id": "11d96092-26f0-4f3c-ae23-dfd1f2e4f5ff"
      },
      "source": [
        "Now we can look at our samples..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a8f798d-28e3-4d20-b341-4c5dee270330",
      "metadata": {
        "id": "8a8f798d-28e3-4d20-b341-4c5dee270330"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(6)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.set_xlim(-1.5, 1.5)\n",
        "ax.set_ylim(-1.5, 1.5)\n",
        "\n",
        "d = train_input[:x.size(0)].detach().to('cpu').numpy()\n",
        "ax.scatter(d[:, 0], d[:, 1], s = 2.5, color = 'gray', label = 'Train')\n",
        "\n",
        "d = x.detach().to('cpu').numpy()\n",
        "ax.scatter(d[:, 0], d[:, 1], s = 2.0, color = 'red', label = 'Synthesis')\n",
        "\n",
        "ax.legend(frameon = False, loc = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38972f3-02e4-4363-9d85-b272db9c2d7d",
      "metadata": {
        "id": "c38972f3-02e4-4363-9d85-b272db9c2d7d"
      },
      "source": [
        "### Back to Stable Diffusion (and its discontents)\n",
        "\n",
        "<img src='https://pbs.twimg.com/media/FasS_3UVsAAOAKh?format=png' width=700/>\n",
        "\n",
        "As discussed, the key differences include...\n",
        "\n",
        "* Diffusion in the latent space\n",
        "* Conditioning/guiding via input prompt\n",
        "* Attention layers to let the network learn how best to use the information in the prompt\n",
        "\n",
        "The amazing results make this look like a solved problem ... the \"ImageNet moment\" of image generation, akin to the breakthroughs of CNNs and Transformers. And in many respects it is.\n",
        "\n",
        "But there are some issues remaining, including\n",
        "* runtime costs and optimality (https://twitter.com/yimatweets/status/1560744922415652864?s=21&t=cKqyctaFgSu96ZVpRxZ57g)\n",
        "* the fact that other noising methods work when key parts of the key theory are not applied\n",
        "  * https://twitter.com/tomgoldsteincs/status/1562503814422630406?s=21&t=paGhXB0cZohusg82uvIjXQ\n",
        "  * https://arxiv.org/abs/2208.09392\n",
        "\n",
        "The latest SOTA (as of September 2022) places these models within a superclass of \"soft score-matching models\" that may address part of these gaps: https://arxiv.org/abs/2208.09392"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperspeed\n",
        "\n",
        "- DALL•E 2 [6 Apr 2022]\n",
        "- Imagen [23 May 2022]\n",
        "- Stable Diffusion [22 Aug 2022]\n",
        "- Make-A-Video [29 Sep 2022]\n",
        "- Imagen-video [6 Oct 2022]\n",
        "\n",
        "And there's more...\n",
        "- Human Motion Diffusion Model https://guytevet.github.io/mdm-page/\n",
        "\n",
        "Longer videos using an alternate approach: https://phenaki.github.io/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qkdOm4JRl0Ae"
      },
      "id": "qkdOm4JRl0Ae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a0a95c8-3115-43ec-93cc-735dd2de06db",
      "metadata": {
        "id": "5a0a95c8-3115-43ec-93cc-735dd2de06db"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}